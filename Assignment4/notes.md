# TODO
- [x] Create separate solver classes ( or at least solver builders )
- [x] Create config files for separate solver classes
- [x] Visualize the policies of the solver
- [x] Q Learning is somehow broken with Frozen Lake
- [x] Create a runtime comparison experiment
- [x] Create a runtime comparison visualization
- [x] Fix the info logging in QLearning so that it has smoothing like VI/PI
- [x] Why does PI on frozen lake go to 1000 iterations?
- [x] Add rolling average and standard deviation to reward plots
- [x] Record final policy for Forest

- [ ] Add an average reward earned to the solvers. Right now reward is the maximum possible reward but its possible that this is hard to achieve
- [ ] Create combined plots if necessary for space
  - [ ] hyper parameter
- [ ] Repeat experiments to capture variation between runs (definitely for QLearning)
- [ ] saved plots every few hundred episodes and turned them into a gif with imageio

# Stream of consciousness


